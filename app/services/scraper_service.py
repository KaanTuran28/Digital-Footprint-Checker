import os
import time
import random
import asyncio
from playwright.async_api import async_playwright

# --- 1. AYARLAR VE SABÄ°TLER ---

LOGIN_CREDENTIALS = {
    "instagram": {
        "username": os.getenv("INSTAGRAM_USERNAME"), 
        "password": os.getenv("INSTAGRAM_PASSWORD")
    },
    "x": {
        "username": os.getenv("X_USERNAME"), 
        "password": os.getenv("X_PASSWORD")
    },
    "linkedin": {
        "username": os.getenv("LINKEDIN_USERNAME"), 
        "password": os.getenv("LINKEDIN_PASSWORD")
    }
}

PLATFORM_CONFIG = {
    "instagram": {
        "login_url": "https://www.instagram.com/accounts/login/",
        "profile_url": "https://www.instagram.com/{}",
        "username_selector": "input[name='username']",
        "password_selector": "input[name='password']",
        "submit_selector": "button[type='submit']"
    },
    "x": {
        "login_url": "https://twitter.com/i/flow/login", # Daha doÄŸrudan login linki
        "profile_url": "https://twitter.com/{}",
        # X giriÅŸinde sÄ±rayla: 1. KullanÄ±cÄ± AdÄ± -> 2. (Bazen) Email/Tel -> 3. Åifre
        "username_selector": "input[autocomplete='username']",
        "verification_selector": "input[data-testid='ocfEnterTextTextInput']", # DoÄŸrulama kutusu
        "password_selector": "input[name='password']"
    },
    "linkedin": {
        "login_url": "https://www.linkedin.com/login",
        "profile_url": "https://www.linkedin.com/in/{}",
        "username_selector": "#username",
        "password_selector": "#password",
        "submit_selector": "button[type='submit']"
    }
}

USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"

# YardÄ±mcÄ± Fonksiyon: Ä°nsan gibi bekleme
async def human_delay(page, min_s=1.0, max_s=3.0):
    await page.wait_for_timeout(random.uniform(min_s, max_s) * 1000)

# --- 2. INSTAGRAM MODÃœLÃœ ---
async def scrape_instagram(context, username, deep_scan):
    print(f"ğŸ“· Instagram taranÄ±yor: {username}")
    page = await context.new_page()
    scraped_texts = []
    config = PLATFORM_CONFIG["instagram"]
    creds = LOGIN_CREDENTIALS["instagram"]
    target_url = config["profile_url"].format(username)

    try:
        # Profile Git
        await page.goto(target_url, wait_until="domcontentloaded", timeout=60000)
        await human_delay(page, 2, 4)

        # GiriÅŸ Gerekli mi?
        if "login" in page.url or await page.locator("input[name='username']").count() > 0:
            print("ğŸ“· Instagram: GiriÅŸ yapÄ±lÄ±yor...")
            if "login" not in page.url: await page.goto(config["login_url"])
            try:
                await page.locator(config["username_selector"]).fill(creds['username'])
                await human_delay(page, 0.5, 1.5)
                await page.locator(config["password_selector"]).fill(creds['password'])
                await human_delay(page, 0.5, 1.5)
                await page.locator(config["submit_selector"]).click()
                await page.wait_for_load_state("networkidle", timeout=30000)
                
                # Pop-up geÃ§me
                try: await page.get_by_role("button", name="Not Now").click()
                except: pass
                try: await page.get_by_role("button", name="Åimdi DeÄŸil").click()
                except: pass
                
                await page.goto(target_url, wait_until="domcontentloaded")
                await human_delay(page, 3, 5)
            except Exception as e:
                print(f"ğŸ“· Instagram giriÅŸ hatasÄ±: {e}")

        # Veri Topla (Biyografi)
        try:
            meta_desc = await page.get_attribute('meta[name="description"]', 'content')
            if meta_desc: scraped_texts.append(f"Ä°STATÄ°STÄ°K: {meta_desc}")
            header_text = await page.locator('header').inner_text()
            clean_header = " | ".join([line.strip() for line in header_text.split('\n') if len(line.strip()) > 1])
            scraped_texts.append(f"BÄ°YOGRAFÄ°: {clean_header}")
        except: pass

        # Veri Topla (Derin Tarama - GÃ¶nderi TÄ±klama)
        if deep_scan:
            print("ğŸ“· Instagram: GÃ¶nderi detaylarÄ±na bakÄ±lÄ±yor...")
            try:
                posts = await page.locator('article a[href^="/p/"]').all()
                for i, post in enumerate(posts[:3]): # Ä°lk 3 gÃ¶nderi
                    try:
                        await post.click()
                        await page.wait_for_selector('div[role="dialog"]', timeout=6000)
                        await human_delay(page, 1, 2)
                        
                        dialog_box = page.locator('div[role="dialog"]')
                        
                        # Konum
                        try:
                            location = await dialog_box.locator('a[href*="/explore/locations/"]').inner_text()
                            scraped_texts.append(f"KONUM: {location}")
                        except: pass

                        # Ä°Ã§erik
                        full_text = await dialog_box.inner_text()
                        clean_text = " ".join([l.strip() for l in full_text.split('\n') if len(l.strip()) > 2]).replace(username, "")
                        scraped_texts.append(f"GÃ–NDERÄ° {i+1}: {clean_text[:1000]}")
                        
                        await page.keyboard.press("Escape")
                        await human_delay(page, 1, 2)
                    except: await page.keyboard.press("Escape")
            except Exception as e: print(f"ğŸ“· Instagram gÃ¶nderi hatasÄ±: {e}")

        final_text = "\n\n".join(scraped_texts)
        if len(final_text) < 10:
             # Fallback
            try: return {"platform": "instagram", "username": username, "data": (await page.locator('body').inner_text())[:3000], "error": None}
            except: return {"platform": "instagram", "error": "Veri Ã§ekilemedi."}

        return {"platform": "instagram", "username": username, "data": final_text, "error": None}

    except Exception as e:
        return {"platform": "instagram", "error": str(e)}
    finally:
        await page.close()

# --- 3. X (TWITTER) MODÃœLÃœ ---
async def scrape_x(context, username, deep_scan):
    print(f"ğŸ¦ X taranÄ±yor: {username}")
    
    # Debug iÃ§in headless=False yapabilirsiniz, canlÄ± izlemek hatayÄ± bulmayÄ± kolaylaÅŸtÄ±rÄ±r
    # page = await context.new_page() 
    
    # X iÃ§in Ã¶zel bir sayfa aÃ§Ä±yoruz
    page = await context.new_page()
    
    scraped_texts = []
    config = PLATFORM_CONFIG["x"]
    creds = LOGIN_CREDENTIALS["x"]
    target_url = config["profile_url"].format(username)

    try:
        # Ã–nce Login OlmayÄ± Dene
        print("ğŸ¦ X: GiriÅŸ sayfasÄ±na gidiliyor...")
        await page.goto(config["login_url"], wait_until="domcontentloaded", timeout=60000)
        await human_delay(page, 3, 5)

        # GiriÅŸ kutusu var mÄ±?
        if await page.locator(config["username_selector"]).count() > 0:
            print("ğŸ¦ X: GiriÅŸ yapÄ±lÄ±yor...")
            
            # 1. ADIM: KullanÄ±cÄ± AdÄ±nÄ± Gir
            await page.locator(config["username_selector"]).fill(creds['username'])
            await page.locator("text=Next").first.click() # Veya "Ä°leri"
            await human_delay(page, 2, 3)

            # 2. ADIM: DoÄŸrulama KontrolÃ¼ (KRÄ°TÄ°K KISIM)
            # Bazen "OlaÄŸandÄ±ÅŸÄ± etkinlik" diyip telefon veya e-posta sorar
            # Genellikle data-testid="ocfEnterTextTextInput" olan bir input Ã§Ä±kar
            verification_input = page.locator("input[data-testid='ocfEnterTextTextInput']")
            
            if await verification_input.count() > 0 and await verification_input.is_visible():
                print("ğŸ¦ X: DoÄŸrulama istendi, e-posta/telefon giriliyor...")
                # Genelde e-posta veya kullanÄ±cÄ± adÄ± ister. 
                # Buraya .env dosyanÄ±zdaki e-postayÄ± veya kullanÄ±cÄ± adÄ±nÄ± tekrar girmeyi deneyebilirsiniz.
                # Åimdilik kullanÄ±cÄ± adÄ±nÄ± tekrar deniyoruz, gerekirse .env'ye EMAIL ekleyip onu kullanÄ±n.
                await verification_input.fill(creds['username']) 
                await page.locator("text=Next").first.click()
                await human_delay(page, 2, 3)

            # 3. ADIM: Åifre Gir
            if await page.locator(config["password_selector"]).count() > 0:
                await page.locator(config["password_selector"]).fill(creds['password'])
                await page.locator("[data-testid='LoginForm_Login_Button']").click()
                await page.wait_for_load_state("networkidle", timeout=30000)
                print("ğŸ¦ X: GiriÅŸ baÅŸarÄ±lÄ± (veya denendi).")
            else:
                print("ğŸ¦ X: Åifre ekranÄ± gelmedi, bir sorun olabilir.")

        # Åimdi Hedef Profile Git
        print(f"ğŸ¦ X: Hedef profile gidiliyor -> {target_url}")
        await page.goto(target_url, wait_until="domcontentloaded", timeout=60000)
        await human_delay(page, 4, 6)

        # Hata KontrolÃ¼ (Hesap yoksa veya gizliyse)
        if await page.locator("text=This account doesnâ€™t exist").count() > 0:
             return {"platform": "x", "username": username, "data": None, "error": "Hesap bulunamadÄ±."}

        # Biyografi Topla
        try:
            # X bazen data-testid'leri deÄŸiÅŸtirir, en garantisi UserDescription
            bio_el = page.locator('[data-testid="UserDescription"]')
            if await bio_el.count() > 0:
                bio = await bio_el.inner_text()
                scraped_texts.append(f"BÄ°YOGRAFÄ°: {bio}")
            
            loc_el = page.locator('[data-testid="UserLocation"]')
            if await loc_el.count() > 0:
                loc = await loc_el.inner_text()
                scraped_texts.append(f"KONUM: {loc}")
                
            # DoÄŸum Tarihi (Varsa)
            birth_el = page.locator('[data-testid="UserBirthdate"]')
            if await birth_el.count() > 0:
                birth = await birth_el.inner_text()
                scraped_texts.append(f"DOÄUM TARÄ°HÄ°: {birth}")
                
        except Exception as e: 
            print(f"ğŸ¦ X Biyografi HatasÄ±: {e}")

        # Tweetler (Deep Scan)
        if deep_scan:
            print("ğŸ¦ X: Tweetler taranÄ±yor...")
            try:
                # SayfayÄ± kaydÄ±r
                for _ in range(3):
                    await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                    await human_delay(page, 2, 3)
                
                # Tweet metinlerini topla
                tweets = await page.locator('[data-testid="tweetText"]').all()
                print(f"ğŸ¦ X: {len(tweets)} adet tweet bulundu.")
                
                for i, tweet in enumerate(tweets[:7]): # Ä°lk 7 tweet
                    text = await tweet.inner_text()
                    # ReklamlarÄ± veya boÅŸ tweetleri ele
                    if len(text) > 5:
                        scraped_texts.append(f"TWEET {i+1}: {text}")
            except Exception as e: 
                print(f"ğŸ¦ X Tweet HatasÄ±: {e}")

        final_text = "\n\n".join(scraped_texts)
        
        # Fallback (Veri yoksa sayfadaki tÃ¼m metni al)
        if len(final_text) < 5: 
            try:
                body_text = await page.locator('body').inner_text()
                return {"platform": "x", "username": username, "data": body_text[:3000], "error": None}
            except:
                return {"platform": "x", "username": username, "data": None, "error": "Veri Ã§ekilemedi, giriÅŸ sorunu olabilir."}
            
        return {"platform": "x", "username": username, "data": final_text, "error": None}

    except Exception as e:
        return {"platform": "x", "error": f"Genel Hata: {str(e)}"}
    finally:
        await page.close()

# --- 4. LINKEDIN MODÃœLÃœ ---
async def scrape_linkedin(context, username, deep_scan):
    print(f"ğŸ‘” LinkedIn taranÄ±yor: {username}")
    page = await context.new_page()
    scraped_texts = []
    config = PLATFORM_CONFIG["linkedin"]
    creds = LOGIN_CREDENTIALS["linkedin"]
    target_url = config["profile_url"].format(username)

    try:
        await page.goto(target_url, wait_until="domcontentloaded", timeout=60000)
        await human_delay(page, 2, 4)

        if "login" in page.url or "authwall" in page.url:
            print("ğŸ‘” LinkedIn: GiriÅŸ yapÄ±lÄ±yor...")
            await page.goto(config["login_url"])
            try:
                await page.locator(config["username_selector"]).fill(creds['username'])
                await page.locator(config["password_selector"]).fill(creds['password'])
                await page.locator(config["submit_selector"]).click()
                await page.wait_for_load_state("networkidle", timeout=30000)
                await page.goto(target_url)
                await human_delay(page, 3, 5)
            except Exception as e: print(f"ğŸ‘” LinkedIn giriÅŸ hatasÄ±: {e}")

        # Profil Bilgileri
        try:
            top_card = await page.locator('.pv-top-card').first.inner_text()
            scraped_texts.append(f"KÄ°MLÄ°K KARTI: {top_card}")
            about = await page.locator('#about').locator('..').inner_text()
            scraped_texts.append(f"HAKKINDA: {about}")
        except: pass

        # GÃ¶nderiler
        if deep_scan:
            try:
                await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                await human_delay(page, 2, 3)
                posts = await page.locator('.feed-shared-update-v2').all()
                for i, post in enumerate(posts[:3]):
                    text = await post.inner_text()
                    scraped_texts.append(f"PAYLAÅIM {i+1}: {text[:500]}")
            except: pass

        final_text = "\n\n".join(scraped_texts)
        if len(final_text) < 5: return {"platform": "linkedin", "username": username, "data": None, "error": "Veri yok"}
        return {"platform": "linkedin", "username": username, "data": final_text, "error": None}

    except Exception as e:
        return {"platform": "linkedin", "error": str(e)}
    finally:
        await page.close()

# --- 5. ANA YÃ–NETÄ°CÄ° (DISPATCHER) ---
async def run_concurrent_scraping(usernames_dict, deep_scan=True):
    async with async_playwright() as p:
        # TarayÄ±cÄ±yÄ± bir kez baÅŸlat (Headless=True: Arka planda, False: GÃ¶rÃ¼nÃ¼r)
        browser = await p.chromium.launch(
            headless=True, 
            args=["--disable-blink-features=AutomationControlled"]
        )
        
        # Ortak Cookie (Oturum) dosyalarÄ±nÄ± yÃ¶netmek iÃ§in context ayarlarÄ±
        # Not: Her platform kendi context'ini fonksiyon iÃ§inde yÃ¶netiyor ama 
        # browser instance'Ä± ortak kullanÄ±lÄ±yor.
        
        tasks = []

        # Instagram
        if usernames_dict.get('instagram'):
            auth_file = "instagram_auth_state.json"
            context = await browser.new_context(
                storage_state=auth_file if os.path.exists(auth_file) else None,
                user_agent=USER_AGENT, viewport={'width': 1366, 'height': 768}
            )
            tasks.append(scrape_instagram(context, usernames_dict['instagram'], deep_scan))

        # X (Twitter)
        if usernames_dict.get('x'):
            auth_file = "x_auth_state.json"
            context = await browser.new_context(
                storage_state=auth_file if os.path.exists(auth_file) else None,
                user_agent=USER_AGENT, viewport={'width': 1366, 'height': 768}
            )
            tasks.append(scrape_x(context, usernames_dict['x'], deep_scan))

        # LinkedIn
        if usernames_dict.get('linkedin'):
            auth_file = "linkedin_auth_state.json"
            context = await browser.new_context(
                storage_state=auth_file if os.path.exists(auth_file) else None,
                user_agent=USER_AGENT, viewport={'width': 1366, 'height': 768}
            )
            tasks.append(scrape_linkedin(context, usernames_dict['linkedin'], deep_scan))

        # Hepsini aynÄ± anda baÅŸlat
        if not tasks: return []
        
        print(f"--- Toplam {len(tasks)} gÃ¶rev baÅŸlatÄ±lÄ±yor... ---")
        results = await asyncio.gather(*tasks)
        
        await browser.close()
        return results